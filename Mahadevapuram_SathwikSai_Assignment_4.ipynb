{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 Assignment: 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "\n",
        "NOTE: The output should be presented well to get **full points**\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1 (20 Points)**"
      ],
      "metadata": {
        "id": "lPTYY22vDnWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**: 20 Newsgroups dataset\n",
        "\n",
        "**Dataset Link**: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
        "\n",
        "**Consider Random 2000 rows only**\n",
        "\n",
        "Generate K=10 topics by using LDA and LSA,\n",
        "then calculate coherence score and determine the optimized K value by the coherence score. Further, summarize and visualize each topics in you own words.\n"
      ],
      "metadata": {
        "id": "dUkAOXJQDq0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Loading the dataset and selecting a random sample of 2000 rows\n",
        "newsgroups_data = fetch_20newsgroups(subset='train')\n",
        "random.seed(42)\n",
        "indices = random.sample(range(len(newsgroups_data.data)), 2000)\n",
        "selected_data = [newsgroups_data.data[i] for i in indices]\n",
        "\n",
        "# Vectorizing the data\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "vectors = vectorizer.fit_transform(selected_data)\n",
        "\n",
        "# Applying LDA\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_topics = lda.fit_transform(vectors)\n",
        "\n",
        "# Calculate the  coherence score for LDA (approximated using topic-word distribution entropy)\n",
        "lda_topic_word_distribution = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
        "lda_coherence_score = -np.sum(lda_topic_word_distribution * np.log(lda_topic_word_distribution + 1e-12)) / lda_topic_word_distribution.shape[0]\n",
        "\n",
        "# Summarize the LDA topics\n",
        "lda_topic_words = []\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
        "    lda_topic_words.append(top_words)\n",
        "\n",
        "# Applying LSA\n",
        "lsa = TruncatedSVD(n_components=10, random_state=42)\n",
        "lsa_topics = lsa.fit_transform(vectors)\n",
        "\n",
        "# Calculate the coherence score for LSA (approximated similarly)\n",
        "lsa_topic_word_distribution = lsa.components_ / lsa.components_.sum(axis=1)[:, np.newaxis]\n",
        "lsa_coherence_score = -np.sum(lsa_topic_word_distribution * np.log(lsa_topic_word_distribution + 1e-12)) / lsa_topic_word_distribution.shape[0]\n",
        "\n",
        "# Summarize the LSA topics\n",
        "lsa_topic_words = []\n",
        "for topic_idx, topic in enumerate(lsa.components_):\n",
        "    top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
        "    lsa_topic_words.append(top_words)\n",
        "\n",
        "# Visualize coherence scores\n",
        "labels = ['LDA', 'LSA']\n",
        "coherence_scores = [lda_coherence_score, lsa_coherence_score]\n",
        "plt.bar(labels, coherence_scores, color=['blue', 'green'])\n",
        "plt.title('Coherence Scores for LDA and LSA')\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.show()\n",
        "\n",
        "# Print summarized topics\n",
        "print(\"LDA Topics:\")\n",
        "for i, words in enumerate(lda_topic_words):\n",
        "    print(f\"Topic {i+1}: {', '.join(words)}\")\n",
        "\n",
        "print(\"\\nLSA Topics:\")\n",
        "for i, words in enumerate(lsa_topic_words):\n",
        "    print(f\"Topic {i+1}: {', '.join(words)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0MjJXg-fpHo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTopic**"
      ],
      "metadata": {
        "id": "nJc0d1D-8FDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents.\n",
        "\n",
        "Dataset from **assignment-3** (text dataset) .\n",
        "\n",
        "> Dont use any custom datasets.\n",
        "\n",
        "\n",
        "> Dataset must have 1000+ rows, no duplicates and null values\n",
        "\n"
      ],
      "metadata": {
        "id": "a4kaHgnnJqQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2 (20 Points)**"
      ],
      "metadata": {
        "id": "KfXGIvg36_P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Q2) **Generate K=10 topics by using BERTopic and then find optimal K value by the coherence score. Interpret each topic and visualize with suitable style.**"
      ],
      "metadata": {
        "id": "38D2s1f77Ebc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BERTopic\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading the dataset using the correct filename\n",
        "file_path = \"barbie_imdb_reviews_cleaned (3).csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Detecting the column with review text\n",
        "print(\" Columns available:\", df.columns.tolist())\n",
        "print(\"\\n Sample data:\\n\", df.head())\n",
        "\n",
        "# Use the first object/text-type column\n",
        "text_column = next((col for col in df.columns if df[col].dtype == 'object'), None)\n",
        "\n",
        "if text_column is None:\n",
        "    raise ValueError(\"No text column found!\")\n",
        "\n",
        "texts = df[text_column].astype(str).tolist()\n",
        "print(f\"\\n Using column: '{text_column}'\")\n",
        "\n",
        "# Setup BERTopic\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom')\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    nr_topics=10,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Step 4: Train Model\n",
        "topics, probs = topic_model.fit_transform(texts)\n",
        "\n",
        "# Step 5: Show Topics and Visualize\n",
        "print(\"\\nðŸ“Š Top 10 Topics:\")\n",
        "print(topic_model.get_topic_info().head(10))\n",
        "\n",
        "# Print keywords for each topic\n",
        "for i in range(10):\n",
        "    print(f\"\\nðŸ”¹ Topic {i} Keywords:\")\n",
        "    print(topic_model.get_topic(i))\n",
        "\n",
        "# Visualizations\n",
        "topic_model.visualize_topics().show()\n",
        "topic_model.visualize_barchart(top_n_topics=10).show()\n",
        "topic_model.visualize_documents(texts).show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NjUfA8LE7OLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3 (25 points)**\n"
      ],
      "metadata": {
        "id": "zoxBIMZfK--R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Link**: 20 Newsgroup Dataset (Random 2000 values)\n",
        "\n",
        "Q3) Using a given dataset, Modify the default representation model by integrating OpenAI's GPT model to generate meaningful summaries for each topic. Additionally, calculate the coherence score to determine the optimal number of topics and retrain the model accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "4lWF1tj96rF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usefull Link: https://maartengr.github.io/BERTopic/getting_started/representation/llm#truncating-documents"
      ],
      "metadata": {
        "id": "3TbQrsW7e4Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai bertopic umap-learn hdbscan scikit-learn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import OpenAI as OpenAIRepresentation\n",
        "import openai\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"SAT140502629\"\n",
        "\n",
        "# Initialize OpenAI representation model for BERTopic\n",
        "representation_model = OpenAIRepresentation(client=openai)\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "docs = fetch_20newsgroups(subset='all').data[:2000]\n",
        "\n",
        "# Setup dimensionality reduction and clustering models\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
        "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom')\n",
        "\n",
        "# Create BERTopic model with OpenAI representation\n",
        "topic_model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    representation_model=representation_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "print(type(docs))\n",
        "print(type(docs[0]))\n",
        "\n",
        "# Show topic info\n",
        "topic_info = topic_model.fit_transform(docs)\n",
        "print(topic_info)\n",
        "\n",
        "# Calculate silhouette score\n",
        "embeddings = topic_model._extract_embeddings(docs, method=\"document\")\n",
        "labels = np.array(topics)\n",
        "valid = labels != -1\n",
        "\n",
        "if len(set(labels[valid])) > 1:\n",
        "    score = silhouette_score(np.array(embeddings)[valid], labels[valid])\n",
        "    print(f\"\\nSilhouette Score: {score:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough valid clusters to compute silhouette score.\")\n",
        "\n",
        "# Example topic print\n",
        "print(\"\\nExample topic summary:\")\n",
        "print(topic_model.get_topic(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e0ZrF3f277gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4 (35 Points)**"
      ],
      "metadata": {
        "id": "c-a-fRBtgI-Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "\n",
        "**BERTopic** allows for extensive customization, including the choice of embedding models, dimensionality reduction techniques, and clustering algorithms.\n",
        "\n",
        "**Dataset Link**: 20 Newsgroup Dataset (Random 2000 values)\n",
        "\n",
        "4)\n",
        "\n",
        "4.1) **Modify the default BERTopic pipeline to use a different embedding model (e.g., Sentence-Transformers) and a different clustering algorithm (e.g., DBSCAN instead of HDBSCAN).\n",
        "\n",
        "4.2: Compare the results of the custom embedding model with the default BERTopic model in terms of topic coherence and interpretability.\n",
        "\n",
        "4.3: Visualize the topics and provide a qualitative analysis of the differences\n",
        "\n",
        "**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usefull Link :https://www.pinecone.io/learn/bertopic/"
      ],
      "metadata": {
        "id": "LYVtmLugexRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic sentence-transformers umap-learn hdbscan scikit-learn gensim plotly --quiet\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Loading 20 Newsgroup Dataset\n",
        "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "docs = np.random.choice(dataset.data, size=2000, replace=False)\n",
        "\n",
        "# Text Cleaning Function\n",
        "def preprocess(texts):\n",
        "    cleaned = []\n",
        "    for doc in texts:\n",
        "        doc = re.sub(r'\\W+', ' ', doc)  # Remove non-word characters\n",
        "        doc = doc.lower()\n",
        "        cleaned.append(doc.strip())\n",
        "    return cleaned\n",
        "\n",
        "cleaned_docs = preprocess(docs)\n",
        "tokenized_docs = [doc.split() for doc in cleaned_docs]\n",
        "\n",
        "# Training the Default BERTopic\n",
        "default_model = BERTopic()\n",
        "default_topics, _ = default_model.fit_transform(cleaned_docs)\n",
        "\n",
        "# Training the Custom BERTopic (DBSCAN + MiniLM)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "custom_model = BERTopic(embedding_model=embedding_model,\n",
        "                        hdbscan_model=DBSCAN(eps=0.3, min_samples=10))\n",
        "custom_topics, _ = custom_model.fit_transform(cleaned_docs)\n",
        "\n",
        "# Calculating the Coherence\n",
        "def compute_coherence(model, tokenized_docs):\n",
        "    topic_info = model.get_topic_info()\n",
        "    topic_ids = topic_info[topic_info.Topic != -1]['Topic'].tolist()\n",
        "\n",
        "    topic_words = []\n",
        "    for topic in topic_ids:\n",
        "        words = [word.lower() for word, _ in model.get_topic(topic)]\n",
        "        topic_words.append(words)\n",
        "\n",
        "    dictionary = Dictionary(tokenized_docs)\n",
        "    coherence_model = CoherenceModel(topics=topic_words, texts=tokenized_docs,\n",
        "                                     dictionary=dictionary, coherence='c_v')\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "# Comparing the Coherence\n",
        "default_coherence = compute_coherence(default_model, tokenized_docs)\n",
        "custom_coherence = compute_coherence(custom_model, tokenized_docs)\n",
        "\n",
        "print(f\"Default BERTopic Coherence: {default_coherence:.4f}\")\n",
        "print(f\"Custom BERTopic (MiniLM + DBSCAN) Coherence: {custom_coherence:.4f}\")\n",
        "\n",
        "# Topic Information\n",
        "print(\"\\nDefault Model Topics:\")\n",
        "print(default_model.get_topic_info().head())\n",
        "\n",
        "print(\"\\nCustom Model Topics:\")\n",
        "print(custom_model.get_topic_info().head())\n",
        "\n",
        "# Visualization\n",
        "default_model.visualize_topics().show()\n",
        "custom_model.visualize_topics().show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XraxAtQP25bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms (LDA, LSA, BERTopic, Modified BERTopic), which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 100 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1) LDA (Latent Dirichlet Allocation)\n",
        " Benefits: Easy to understand, popular.\n",
        " Cons: Assumes word independence, has trouble with big datasets, and requires predetermined topic numbers.\n",
        " Ideal for: Clearly distributed topics in small to medium-sized datasets.\n",
        "\n",
        " 2) LSA(Latent Semantic Analysis)\n",
        " Advantages: Does not require predetermined subjects and captures latent structures in text.\n",
        " Cons: Less effective for complex issues, linear assumptions, poor interpretability.\n",
        " Ideal for: Latent associations in medium-sized datasets.\n",
        "\n",
        " 3) BERTopic:\n",
        " Advantages: scalable for big datasets, eliminates the requirement to predefine topics, and uses BERT embeddings for improved topic context.\n",
        " Cons: Depends on the quality of the embeddings and is computationally costly.\n",
        " Ideal for: Big datasets with intricate, situational language.\n",
        "\n",
        "4) Modified BERTopic:\n",
        "Advantages: High-quality, human-readable subjects with GPT summaries for improved topic interpretability.\n",
        "Cons: Because GPT is used, it is expensive and resource-intensive.\n",
        "Ideal for: Big datasets that require thorough, understandable topic summaries.\n",
        "\n",
        "Best Algorithm:\n",
        "1) Modified  While BERTopic requires a lot of resources, it is the greatest option overall for producing high-quality, accessible summaries.\n",
        "2) Large datasets with contextual information are most suited for BERTopic.\n",
        "3) LDA works best on small, straightforward datasets.\n",
        "4) Although it is more difficult to interpret, LSA is effective for medium-sized datasets with latent structures.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Learning Experience: Gaining practical knowledge of how text data can be handled and analyzed was made possible by working with topic modeling algorithms like LDA, LSA, BERTopic, and Modified BERTopic.  I learned more about the various approaches to topic extraction and how their performance and complexity vary.  I was able to better understand important ideas like probabilistic modeling (LDA), dimensionality reduction (LSA), and the application of transformer-based models for contextual topic extraction (BERTopic) thanks to the implementations.  The chance to adapt BERTopic by including OpenAI's GPT for subject summaries was especially illuminating since it demonstrated how sophisticated models can improve the interpretability of outcomes.\n",
        "\n",
        "Challenges Encountered:\n",
        "1) Algorithm Complexity.\n",
        "2) Data Preprocessing.\n",
        "3) Computaional Resources.\n",
        "4) Error Troubleshooting.\n",
        "\n",
        "Relevance to Your Field of Study: Natural Language Processing (NLP), in particular, is a very pertinent area of study for me.  A fundamental method in NLP for evaluating and condensing vast volumes of text input is topic modeling.  Understanding the various methods (such LDA and BERTopic) and how to extract significant features is directly applicable to real-world problems like information retrieval, sentiment analysis, and document clustering.\n",
        "\n",
        "All things considered, this assignment gave me a better grasp of topic modeling algorithms' operation, implementation difficulties, and connections to more complex NLP tasks.  It improved my ability to solve problems, particularly when it came to debugging and optimizing models for real-world applications.  Additionally, it reaffirmed the significance of careful parameter tweaking and computational resources when dealing with big text corpora.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}